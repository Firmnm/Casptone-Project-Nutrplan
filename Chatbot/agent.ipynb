{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3d1ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\KULIAH\\SMT 6\\Fintune model langachain\\venv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import PrivateAttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53974d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(folder_path):\n",
    "    text = \"\"\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            try:\n",
    "                reader = PdfReader(os.path.join(folder_path, file))\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Gagal memproses {file}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f939f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return FAISS.from_texts(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0e4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_retriever(path, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    index = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
    "    return index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2afd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    _model: any = PrivateAttr()\n",
    "    _tokenizer: any = PrivateAttr()\n",
    "    _device: str = PrivateAttr(default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self, model, tokenizer, device=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._model = model\n",
    "        self._tokenizer = tokenizer\n",
    "        self._device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom-mistral-lora\"\n",
    "\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(self._device)\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            del inputs[\"token_type_ids\"]\n",
    "        outputs = self._model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.9,       # tingkatkan supaya output lebih variatif\n",
    "            top_p=0.95,            # sampling yang lebih luas\n",
    "            do_sample=True,\n",
    "            pad_token_id=self._tokenizer.eos_token_id\n",
    "        )\n",
    "        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Hapus prompt dari hasil output agar hanya jawaban yang keluar\n",
    "        return response[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d623e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_retriever(query, retrievers):\n",
    "    docs = []\n",
    "    for retriever in retrievers:\n",
    "        if retriever:\n",
    "            docs += retriever.get_relevant_documents(query)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8325a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_food_or_drink(query: str) -> str:\n",
    "    \"\"\"Klasifikasi sederhana untuk membedakan makanan atau minuman.\"\"\"\n",
    "    minuman_keywords = [\"susu\", \"teh\", \"kopi\", \"air\", \"jus\", \"sirup\", \"soda\", \"minuman\", \"milk\", \"drink\"]\n",
    "    makanan_keywords = [\"nasi\", \"roti\", \"daging\", \"telur\", \"sayur\", \"makanan\", \"makan\", \"ayam\", \"ikan\", \"mie\", \"rice\", \"food\", \"bread\"]\n",
    "\n",
    "    query_lower = query.lower()\n",
    "    if any(word in query_lower for word in minuman_keywords):\n",
    "        return \"minuman\"\n",
    "    elif any(word in query_lower for word in makanan_keywords):\n",
    "        return \"makanan\"\n",
    "    else:\n",
    "        return \"tidak diketahui\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7baa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, llm, retrievers):\n",
    "    jenis = classify_food_or_drink(query)\n",
    "\n",
    "    docs = combined_retriever(query, retrievers)\n",
    "\n",
    "    if docs:\n",
    "        context = \"\\n\\n---\\n\\n\".join(\n",
    "            [f\"Sumber: {d.metadata.get('source', 'unknown')}\\nKonten: {d.page_content}\" for d in docs]\n",
    "        )\n",
    "        prompt = f\"\"\"\n",
    "Anda adalah asisten nutrisi yang sangat membantu dan informatif.\n",
    "\n",
    "Jenis bahan dalam pertanyaan ini adalah: {jenis}.\n",
    "\n",
    "Jawab pertanyaan berdasarkan konteks berikut:\n",
    "\n",
    "{context}\n",
    "\n",
    "Pertanyaan: {query}\n",
    "\n",
    "Jawaban:\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "Anda adalah asisten nutrisi yang sangat membantu dan informatif.\n",
    "\n",
    "Jenis bahan dalam pertanyaan ini adalah: {jenis}.\n",
    "\n",
    "Jawab pertanyaan berikut berdasarkan pengetahuan umum Anda.\n",
    "\n",
    "Pertanyaan: {query}\n",
    "\n",
    "Jawaban:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Prompt yang dikirim ke model:\\n\", prompt)  # Debug\n",
    "    answer = llm._call(prompt)\n",
    "    return answer, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ae0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan indeks FAISS lokal yang sudah ada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:27<00:00, 13.53s/it]\n",
      "C:\\Users\\Farhan Rahmansyah\\AppData\\Local\\Temp\\ipykernel_11072\\3852927642.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    }
   ],
   "source": [
    "# ===== Konfigurasi Path dan Model =====\n",
    "pdf_folder = \"WHO_doc\"  # Folder berisi PDF WHO/FAO\n",
    "index_path = \"faiss_who_index\"\n",
    "adapter_path = \"./mistral-lora-adapter\"  # Path ke adapter LoRA\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# ===== Cek dan Bangun FAISS Index jika belum ada =====\n",
    "if not os.path.exists(index_path):\n",
    "    print(\"Membangun indeks FAISS dari dokumen PDF...\")\n",
    "    raw_text = extract_text_from_pdfs(pdf_folder)\n",
    "    index = build_faiss_index(raw_text)\n",
    "    if index:\n",
    "        index.save_local(index_path)\n",
    "        print(\"Indeks FAISS berhasil dibuat dan disimpan.\")\n",
    "    else:\n",
    "        print(\"Gagal membangun indeks FAISS.\")\n",
    "else:\n",
    "    print(\"Menggunakan indeks FAISS lokal yang sudah ada.\")\n",
    "\n",
    "# ===== Load Tokenizer dan Model =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "llm = CustomLLM(model, tokenizer)\n",
    "\n",
    "# ===== Load FAISS Retriever =====\n",
    "retriever = load_faiss_retriever(index_path)\n",
    "retrievers = [retriever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5fa5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pertanyaan: Berapa kandungan nutrisi dalam air?\n",
      "\n",
      "Prompt yang dikirim ke model:\n",
      " \n",
      "Anda adalah asisten nutrisi yang sangat membantu dan informatif.\n",
      "\n",
      "Jenis bahan dalam pertanyaan ini adalah: minuman.\n",
      "\n",
      "Jawab pertanyaan berdasarkan konteks berikut:\n",
      "\n",
      "Sumber: unknown\n",
      "Konten: (https://extranet.who.int/nutrition/gina/en/node/41857, accessed 18Â January 2023).\n",
      "101. Government of Finland. Peraturan Menteri Kesehatan Nomor 30 tentang pencantuman informasi andijngan gula, garam, \n",
      "dan lemak serta pesan kesehatan untuk pangan olahan dan pangan siap saji [Inclusion of sugar, salt and fat contents and \n",
      "health message on processed and fast foods] 2013 (https://extranet.who.int/nutrition/gina/en/node/26167).\n",
      "102. Peraturan Menteri Kesehatan Nomor 30 tentang pencantuman informasi andijngan gula, garam, dan lemak serta pesan \n",
      "kesehatan untuk pangan olahan dan pangan siap saji [Inclusion of sugar, salt and fat contents and health message \n",
      "on processed and fast foods]. Jakarta: Government of Indonesia; 2017 (https://extranet.who.int/nutrition/gina/en/\n",
      "node/26167, accessed 18Â January 2023).\n",
      "103. è¥å…»å¥åº·é¤åŽ…å»ºè®¾æŒ‡å— [Guidelines for building a nutritious and healthy restaurant]. Beijing: Government of China; 2020\n",
      "\n",
      "---\n",
      "\n",
      "Sumber: unknown\n",
      "Konten: providing exercise and changing facilities, adopting healthy nutriÂ­\n",
      "tion catering standards, and initiating other appropriate schemes. \n",
      "â€¢ Interventions aimed at the prevention and management of obesity \n",
      "should be carefully designed so that they do not cause undue fear of \n",
      "fatness and precipitate eating disorders. \n",
      "â€¢ Consumers should be educated and encouraged to demand food \n",
      "products of high nutritional quality. \n",
      "â€¢ The strategies adopted should be population-specific, especially \n",
      "with respect to economic circumstances. Thus, for example, the \n",
      "main aim of physical activity interventions in developing countries \n",
      "should be to prevent the decline in such activity that usually accomÂ­\n",
      "panies economic development, whereas the main emphasis in affluÂ­\n",
      "ent societies should be on discouraging existing patterns of sedenÂ­\n",
      "tary behaviour. \n",
      "Need for public health strategies \n",
      "Population-based (universal) public health strategies should be\n",
      "\n",
      "---\n",
      "\n",
      "Sumber: unknown\n",
      "Konten: The Codex Alimentarius --- the intergovernmental standard-setting body\n",
      "through which nations agree on standards for food --- is currently being\n",
      "reviewed. Its work in the area of nutrition and labelling could be further\n",
      "strengthened to cover diet-related aspects of health. The feasibility of\n",
      "codes of practice in food advertising should also be explored.\n",
      "142\n",
      "6.4.4 Ensuring that â€˜â€˜healthy dietâ€™â€™ components are available to all\n",
      "As consumers increase their preference for healthy diets, producers and\n",
      "suppliers will wish to orient their products and marketing to respond to\n",
      "this emerging demand. Governments could make it easier for consumersto exercise healthier choices, in accordance with the population nutrient\n",
      "intake goals given in this report by, for example, promoting the wider\n",
      "availability of food which is less processed and low in trans fatty acids,\n",
      "encouraging the use of vegetable oil for domestic consumers, and\n",
      "ensuring an adequate and sustainable supply of fish, fruits, vegetables\n",
      "\n",
      "Pertanyaan: Berapa kandungan nutrisi dalam air?\n",
      "\n",
      "Jawaban:\n",
      "\n",
      "ðŸ’¬ Jawaban:\n",
      " Air memiliki kandungan nutrisi sebagai berikut: 0.0 kalori, 0.0g protein, 0.0g lemak, dan 0.0g karbohidrat. Makanan ini termasuk dalam kategori Tidak Sehat.\n"
     ]
    }
   ],
   "source": [
    "# ===== Fungsi utama untuk menjawab pertanyaan =====\n",
    "def main():\n",
    "    question = \"Berapa kandungan nutrisi dalam air?\"\n",
    "    print(f\"\\nPertanyaan: {question}\\n\")\n",
    "\n",
    "    answer, docs = answer_query(question, llm, retrievers)\n",
    "    print(\"ðŸ’¬ Jawaban:\\n\", answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
