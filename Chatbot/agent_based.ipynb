{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857cb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import PrivateAttr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ee893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(folder_path):\n",
    "    text = \"\"\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            try:\n",
    "                reader = PdfReader(os.path.join(folder_path, file))\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "            except Exception as e:\n",
    "                print(f\"Gagal memproses {file}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9262ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "def load_faiss_retriever(path, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    index = FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)\n",
    "    return index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "class DuckDuckGoRetriever:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        if ddg is None:\n",
    "            print(\"âŒ Modul duckduckgo-search tidak tersedia.\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = ddg(query, max_results=self.k)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ DuckDuckGo error: {e}\")\n",
    "            return []\n",
    "        \n",
    "        if not results:\n",
    "            print(\"âš ï¸ Tidak ada hasil dari DuckDuckGo.\")\n",
    "            return []\n",
    "\n",
    "        docs = []\n",
    "        for r in results:\n",
    "            content = (\n",
    "                r.get(\"body\") or \n",
    "                r.get(\"text\") or \n",
    "                r.get(\"title\") or \n",
    "                r.get(\"href\") or \"\"\n",
    "            )\n",
    "            if content:\n",
    "                print(f\"ðŸŒ [DuckDuckGo] {content}\")\n",
    "                docs.append(Document(page_content=content, metadata={\"source\": r.get(\"href\", \"duckduckgo\")}))\n",
    "        \n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    _model: any = PrivateAttr()\n",
    "    _tokenizer: any = PrivateAttr()\n",
    "    _device: str = PrivateAttr(default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self, model, tokenizer, device=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._model = model\n",
    "        self._tokenizer = tokenizer\n",
    "        self._device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom-mistral-lora\"\n",
    "\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(self._device)\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            del inputs[\"token_type_ids\"]\n",
    "        outputs = self._model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self._tokenizer.eos_token_id\n",
    "        )\n",
    "        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response[len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_retriever(query, retrievers):\n",
    "    docs = []\n",
    "    for retriever in retrievers:\n",
    "        if retriever:\n",
    "            docs += retriever.get_relevant_documents(query)\n",
    "    return docs\n",
    "\n",
    "def answer_query(query, llm, retrievers):\n",
    "    docs = combined_retriever(query, retrievers)\n",
    "    if not docs:\n",
    "        return \"Maaf, tidak ada informasi relevan ditemukan.\", []\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join([f\"Sumber: {d.metadata.get('source', 'unknown')}\\nKonten: {d.page_content}\" for d in docs])\n",
    "    prompt = f\"\"\"\n",
    "Anda adalah virtual asisten nutrisi yang informatif. Berdasarkan konteks berikut, jawab pertanyaan pengguna. Jika tidak yakin, katakan tidak tahu.\n",
    "\n",
    "[KONTEKS]\n",
    "{context}\n",
    "\n",
    "[PERTANYAAN]\n",
    "{query}\n",
    "\n",
    "[JAWABAN ANDA]\n",
    "\"\"\"\n",
    "    answer = llm._call(prompt)\n",
    "    return answer, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6788f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi path dan model\n",
    "pdf_folder = \"WHO_doc\"  # Folder berisi PDF WHO/FAO\n",
    "index_path = \"faiss_who_index\"\n",
    "adapter_path = \"./mistral-lora-adapter\"  # Path ke adapter LoRA\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Cek apakah FAISS index sudah ada, jika belum buat dari PDF\n",
    "if not os.path.exists(index_path):\n",
    "    raw_text = extract_text_from_pdfs(pdf_folder)\n",
    "    index = build_faiss_index(raw_text)\n",
    "    if index:\n",
    "        index.save_local(index_path)\n",
    "    else:\n",
    "        print(\"Gagal membangun indeks FAISS.\")\n",
    "else:\n",
    "    print(\"Menggunakan indeks FAISS lokal yang sudah ada.\")\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Konfigurasi quantization untuk efisiensi memori\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load base model dengan konfigurasi quantized\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# Load adapter LoRA ke model dasar\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Bungkus model dan tokenizer dalam custom class\n",
    "llm = CustomLLM(model, tokenizer)\n",
    "\n",
    "# Load retriever: FAISS lokal + fallback DuckDuckGo\n",
    "retrievers = [\n",
    "    load_faiss_retriever(index_path),\n",
    "    DuckDuckGoRetriever()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4331261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi bantu untuk buat prompt\n",
    "def generate_prompt(context, question):\n",
    "    return f\"\"\"Berikut ini adalah konteks dari dokumen WHO/FAO:\\n\\n{context}\\n\\nPertanyaan: {question}\\nJawaban:\"\"\"\n",
    "\n",
    "# Fungsi untuk menggabungkan hasil dari dua retriever\n",
    "def combine_retrieval_results(retrievers, query, top_k=3):\n",
    "    docs = []\n",
    "    for retriever in retrievers:\n",
    "        try:\n",
    "            result = retriever.get_relevant_documents(query)\n",
    "            if result:\n",
    "                docs.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal menggunakan retriever {retriever.__class__.__name__}: {e}\")\n",
    "    # Hapus duplikat berdasarkan konten\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    for doc in docs:\n",
    "        if doc.page_content not in seen:\n",
    "            seen.add(doc.page_content)\n",
    "            unique_docs.append(doc)\n",
    "    return unique_docs[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea74669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str):\n",
    "    print(f\"\\nPertanyaan: {question}\\n\")\n",
    "\n",
    "    # Ambil dokumen dari semua retriever (FAISS + DuckDuckGo)\n",
    "    documents = combine_retrieval_results(retrievers, question, top_k=3)\n",
    "\n",
    "    if not documents:\n",
    "        return \"Maaf, saya tidak menemukan jawaban relevan dari dokumen.\"\n",
    "\n",
    "    # Gabungkan konteks dari dokumen\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    # Bangun prompt dengan konteks\n",
    "    prompt = generate_prompt(context=context, question=question)\n",
    "\n",
    "    # Tokenisasi\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate jawaban\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode jawaban\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Hilangkan prompt dari jawaban\n",
    "    final_answer = answer.replace(prompt, \"\").strip()\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Apa saja makanan yang cocok untuk diet?\"\n",
    "jawaban, docs = answer_query(question, llm, retrievers)\n",
    "print(\"\\nðŸ’¬ Jawaban:\\n\", jawaban)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
